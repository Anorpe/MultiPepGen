# -*- coding: utf-8 -*-
"""Utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UwpiBdPTS2Ia95YMIkzysUOB6vnwfwIV

# Libraries install
"""

!pip install autokeras

import autokeras

!pip install biopython modlamp gensim biovec pydpi pyspark

"""
# Libraries

"""

#Utils
import numpy as np
import sys
import matplotlib.pyplot as plt
import pandas as pd
import joblib
from time import time
import datetime

# Preprocesing
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder



#RNN WGAN
from __future__ import print_function, division
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D,LSTM,TimeDistributed,RepeatVector,TimeDistributed
#from keras.layers.advanced_activations import LeakyReLU
from keras.layers import LeakyReLU
#from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from keras.layers import Bidirectional

#Spark
from bs4 import BeautifulSoup
import requests
import os
from pyspark.sql import SparkSession

"""# Constants"""

LABELS = [
          'microbiano',
          'bacteriano',
          'antigramneg',
          'antigrampos',
          'fungico',
          'viral',
          'cancer'
          ]

data_sample = pd.read_csv("output/data_columns_sample.csv")
columns = list(data_sample.columns)

data_sample = pd.read_csv("output/data_robust_selected_columns_sample.csv")
selection_robust = list(data_sample.columns)
selection_robust.remove('sequence')

data_sample = pd.read_csv("output/data_minmax_selected_columns_sample.csv")
selection_minmax = list(data_sample.columns)
selection_minmax.remove('sequence')

"""# Functions"""

def df_to_fasta(df,path):
  with open(path,'w') as file:
    con = 0
    for index, row in df.iterrows():
      file.write('>SEQ'+str(con)+'\n')
      file.write(str(row['sequence'])+"\n")
      con += 1

def list_to_fasta(lista,path):
  with open(path,'w') as file:
    con = 0
    for row in lista:
      file.write('>SEQ'+str(con)+'\n')
      file.write(str(row)+"\n")
      con += 1


def time_function(f,*args,**kwargs):
  start_time = time()
  # Take the original function's return value.
  ret = f(*args,**kwargs)
  # Calculate the elapsed time.
  elapsed_time = time() - start_time
  print("Elapsed time: %0.10f seconds." % elapsed_time)
  return ret



def complete_coding(matrix):
    agg = MAX_LEN - len(matrix)
    zeros = [0]*len(valid_aminoacids)
    if(type(matrix)!=type([])):
        matrix = list(matrix)
    for i in range(agg):
        matrix.append(zeros)

    return matrix

def complete_sequence(sequence):
    agregate = int(MAX_LEN - len(sequence))

    return sequence + "_"*agregate


def escalon(array):
  maximo = max(array)
  escalon = []
  for i in array:
    if i == maximo:
      escalon.append(1)
    else:
      escalon.append(0)

  return np.array(escalon)

def escalon_matrix(matrix):
  escalon_matrix = []
  for array in matrix:
    escalon_matrix.append(np.array(escalon(array)))
  return escalon_matrix


def get_input_generator(seq):
  #complete sequence with _
  seq = complete_sequence(seq)
  #coding
  coding = ohe.transform(np.array(list(seq)).reshape(-1,1))
  #flatten coding
  flatten = []
  for i in range(35):
    for j in range(21):
      flatten.append(int(coding[i][j]))
  flatten = np.array(flatten).reshape(1,-1)

  return flatten


def flatten(coding):
  flatten = []
  for i in range(35):
    for j in range(21):
      flatten.append(int(coding[i][j]))
  flatten = np.array(flatten).reshape(1,-1)

  return flatten

def encoding_neg(data):
  sequences = list(complete_sequence(i) for i in data[data['class']==0]['sequence'])
  ohes = []
  for sequence in sequences:

      coding = ohe.transform(np.array(list(sequence)).reshape(-1,1))
      ohes.append(coding)

  return np.array(ohes)

def encoding_pos(data):
  sequences = list(complete_sequence(i) for i in data[data['class']==1]['sequence'])
  ohes = []
  for sequence in sequences:

      coding = ohe.transform(np.array(list(sequence)).reshape(-1,1))
      ohes.append(coding.reshape((35,21,1)))

  return np.array(ohes)

def encoding(data):
  sequences = list(complete_sequence(i) for i in data['sequence'])
  ohes = []
  for sequence in sequences:

      coding = ohe.transform(np.array(list(sequence)).reshape(-1,1))
      ohes.append(coding.reshape((35,21,1)))

  return np.array(ohes,dtype = "float32")


def minmax_norm(df_input):
    return (df - df.min()) / ( df.max() - df.min())

def mean_norm(df_input):
    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)


def quantile_norm(df_input):
    sorted_df = pd.DataFrame(np.sort(df_input.values,axis=0), index=df_input.index, columns=df_input.columns)
    mean_df = sorted_df.mean(axis=1)
    mean_df.index = np.arange(1, len(mean_df) + 1)
    quantile_df =df_input.rank(method="min").stack().astype(int).map(mean_df).unstack()
    return(quantile_df)

"""## Filter sequences"""

dicti = {}
valid_aas = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P','S','T','W','Y', 'V'] #+ ['B','Z','J']
valid_aas = set(valid_aas)
def amp_filter(seq, valid_aas = valid_aas , min_aas = 7, max_aas = 35, min_unique_aas = 3, verbose = False):
    """
    Input:
        -seq: aminoacid sequence.
        -min_aas: minimum number of aas for sequence.
        -max_aas: maximum number of aas for sequence.
        -valid_aas: a list with the aminoacids to be included.
        -min_unique_aas = minimum number of unique aas for sequence.

    Output:
        >False if 'seq' does not comply with any of the criteria.
        >True if 'seq' complies with all of the criteria.
    """

    if len(seq) > max_aas or len(seq) < min_aas:
        if verbose:
            print("Discarded ", seq, " due to length ", len(seq))
        return False
    set_diff = set(seq)-valid_aas
    if len(set_diff)>0:
        if verbose:
            print("Discarded ", seq, " due to ", set_diff)
        return False


    if len(set(seq)) < min_unique_aas:
        if verbose:
            print("Discarded ", seq, " due to low number of different aas")
        return False
    return True


def replace_bzj(seq, verbose = False):
    res = seq.replace('B', random.sample(['N','D'], 1)[0])
    res = res.replace('Z', random.sample(['Q','E'], 1)[0])
    res = res.replace('J', random.sample(['I','L'], 1)[0])
    if res != seq and verbose:
        print(seq, "--->", res)
    return res


def amp_filter_df(df, valid_aas = valid_aas , min_aas = 7, max_aas = 35, min_unique_aas = 3, verbose = False):
  df_result = pd.DataFrame({})
  for index, sequence in df.iterrows():
    try:
      if amp_filter(sequence["sequence"]):
          df_result = df_result.append({"sequence":sequence["sequence"]},ignore_index = True)
    except:
      print("Hub un error filtrando la secuencia: ", sequence["sequence"], "type: ", type(sequence["sequence"]))
  df_result = df_result.drop_duplicates(["sequence"])
  return df_result

"""
## Calculation of descriptors"""

#Biology Libraries
from Bio.SeqUtils.ProtParam import ProteinAnalysis
from modlamp.descriptors import GlobalDescriptor
from Bio import SeqIO

#Biology Libraries
from Bio.SeqUtils.ProtParam import ProteinAnalysis
from modlamp.descriptors import GlobalDescriptor
from Bio import SeqIO
from biovec.models import prot_vec
import collections
from collections import Counter

#Se importa la librería pydpi de Python 2.
#Para que esto funcione, se debe crear una carpeta "tmp" en la raíz del disco donde se esté corriendo el programa.
from past import autotranslate
autotranslate(['pydpi'])
import pydpi
from pydpi.protein import AAComposition
from pydpi.protein import CTD
from pydpi.protein import Autocorrelation
from pydpi.protein import QuasiSequenceOrder
from pydpi.protein import PseudoAAC



pv2 = prot_vec.load_protvec('models/swissprot-reviewed-protvec.model')


def get_GAAC(seq):
    group = {
        'GAAC_aliphatic': 'GAVLMI',
        'GAAC_aromatic': 'FYW',
        'GAAC_positivecharge': 'KRH',
        'GAAC_negativecharge': 'DE',
        'GAAC_uncharged': 'STCPNQ'
    }

    fts = {}

    groupKey = group.keys()
    count = Counter(seq)
    myDict = {}

    for key in groupKey:
        for aa in group[key]:
            myDict[key] = myDict.get(key, 0) + count[aa]
    for key in groupKey:
        fts[key]=(myDict[key]/len(seq))
    return fts


def get_GDPC(seq):
    group = {
        'aliphatic': 'GAVLMI',
        'aromatic': 'FYW',
        'positivecharge': 'KRH',
        'negativecharge': 'DE',
        'uncharged': 'STCPNQ'
    }

    groupKey = group.keys()
    baseNum = len(groupKey)
    dipeptide = [g1 + '.' + g2 for g1 in groupKey for g2 in groupKey]

    index = {}
    for key in groupKey:
        for aa in group[key]:
            index[aa] = key

    fts = {}



    myDict = {}
    for t in dipeptide:
        myDict[t] = 0

    sum = 0
    for j in range(len(seq) - 2 + 1):
        myDict[index[seq[j]] + '.' + index[seq[j + 1]]] = myDict[index[seq[j]] + '.' + index[
            seq[j + 1]]] + 1
        sum = sum + 1

    if sum == 0:
        for t in dipeptide:
            fts['GDPC_'+t]=0
            #code.append(0)
    else:
        for t in dipeptide:

            fts['GDPC_'+t]=(myDict[t] / sum)

    return fts

def get_GTPC(seq):
    group = {
        'aliphatic': 'GAVLMI',
        'aromatic': 'FYW',
        'positivecharge': 'KRH',
        'negativecharge': 'DE',
        'uncharged': 'STCPNQ'
    }

    groupKey = group.keys()
    baseNum = len(groupKey)
    triple = [g1+'.'+g2+'.'+g3 for g1 in groupKey for g2 in groupKey for g3 in groupKey]

    index = {}
    for key in groupKey:
        for aa in group[key]:
            index[aa] = key

    fts = {}
    myDict = {}
    for t in triple:
        myDict[t] = 0
    sum = 0
    for j in range(len(seq) - 3 + 1):
        myDict[index[seq[j]]+'.'+index[seq[j+1]]+'.'+index[seq[j+2]]] = myDict[index[seq[j]]+'.'+index[seq[j+1]]+'.'+index[seq[j+2]]] + 1
        sum = sum +1
    if sum == 0:
        for t in triple:
            fts['GTPC_'+t]=0
    else:
        for t in triple:
            fts['GTPC_'+t]=(myDict[t]/sum)

    return fts

def get_grouped_aa_features(features):
    gaac = get_GAAC(features['sequence'])
    gdpc = get_GDPC(features['sequence'])
    gdtp = get_GTPC(features['sequence'])
    features.update(gaac)
    features.update(gdpc)
    features.update(gdtp)
    return features

def get_global_features(features):
    seq = features['sequence']
    desc = GlobalDescriptor(seq)
    desc.calculate_MW(amide=False)
    features['molecular_weight'] = desc.descriptor[0][0]
    desc.calculate_charge(ph=7.0)
    features['charge'] = desc.descriptor[0][0]
    desc.charge_density(ph = 7.0)
    features['charge_density'] = desc.descriptor[0][0]
    #desc.isoelectric_point(amide=False)
    #features['isoelectric_point'] = desc.descriptor[0][0]
    biop_analysis = ProteinAnalysis(seq)
    features['isoelectric_point'] = biop_analysis.isoelectric_point()
    #features['flexibility'] = biop_analysis.flexibility()
    features['gravy'] = biop_analysis.gravy()
    desc.instability_index()
    features['instability_index'] = desc.descriptor[0][0]
    desc.aromaticity()
    features['aromaticity'] = desc.descriptor[0][0]
    desc.aliphatic_index()
    features['aliphatic_index'] = desc.descriptor[0][0]
    desc.boman_index()
    features['boman_index'] = desc.descriptor[0][0]
    desc.hydrophobic_ratio()
    features['hydrophobic_ratio'] = desc.descriptor[0][0]
    return features

def get_composition_features(features):
    seq = features['sequence']